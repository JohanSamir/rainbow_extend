Loaded trained dqn in cartpole
Training fixed agent 1, please be patient, may be a while...
I0910 10:46:37.511094 140527592425472 run_experiment.py:549] Creating TrainRunner ...
WARNING:tensorflow:From /home/joaoguilhermearujo/miniconda3/envs/rain/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
W0910 10:46:37.511543 140527592425472 deprecation.py:345] From /home/joaoguilhermearujo/miniconda3/envs/rain/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
I0910 10:46:37.561826 140527592425472 dqn_agent.py:271] Creating JaxDQNAgentNew agent with the following parameters:
I0910 10:46:37.562525 140527592425472 dqn_agent.py:272] 	 gamma: 0.990000
I0910 10:46:37.562581 140527592425472 dqn_agent.py:273] 	 update_horizon: 1.000000
I0910 10:46:37.562651 140527592425472 dqn_agent.py:274] 	 min_replay_history: 500
I0910 10:46:37.562701 140527592425472 dqn_agent.py:275] 	 update_period: 4
I0910 10:46:37.562752 140527592425472 dqn_agent.py:276] 	 target_update_period: 100
I0910 10:46:37.562797 140527592425472 dqn_agent.py:277] 	 epsilon_train: 0.010000
I0910 10:46:37.562944 140527592425472 dqn_agent.py:278] 	 epsilon_eval: 0.001000
I0910 10:46:37.562994 140527592425472 dqn_agent.py:279] 	 epsilon_decay_period: 250000
I0910 10:46:37.563059 140527592425472 dqn_agent.py:280] 	 optimizer: adam
I0910 10:46:37.563130 140527592425472 dqn_agent.py:282] 	 max_tf_checkpoints_to_keep: 4
I0910 10:46:37.563189 140527592425472 dqn_agent.py:283] 	 seed: 1631270797561787
I0910 10:46:37.564478 140527592425472 circular_replay_buffer.py:155] Creating a OutOfGraphPrioritizedReplayBuffer replay memory with the following parameters:
I0910 10:46:37.564599 140527592425472 circular_replay_buffer.py:156] 	 observation_shape: (4, 1)
I0910 10:46:37.564649 140527592425472 circular_replay_buffer.py:157] 	 observation_dtype: <class 'jax._src.numpy.lax_numpy.float64'>
I0910 10:46:37.564817 140527592425472 circular_replay_buffer.py:158] 	 terminal_dtype: <class 'numpy.uint8'>
I0910 10:46:37.564871 140527592425472 circular_replay_buffer.py:159] 	 stack_size: 1
I0910 10:46:37.564924 140527592425472 circular_replay_buffer.py:160] 	 replay_capacity: 50000
I0910 10:46:37.564969 140527592425472 circular_replay_buffer.py:161] 	 batch_size: 128
I0910 10:46:37.565027 140527592425472 circular_replay_buffer.py:162] 	 update_horizon: 1
I0910 10:46:37.565087 140527592425472 circular_replay_buffer.py:163] 	 gamma: 0.990000
I0910 10:46:38.775001 140527592425472 dqn_agent.py:70] Creating Adam optimizer with settings lr=0.001000, beta1=0.900000, beta2=0.999000, eps=0.000313
I0910 10:46:38.857325 140527592425472 dqn_agent.py:271] Creating JaxDQNAgentNew agent with the following parameters:
I0910 10:46:38.857498 140527592425472 dqn_agent.py:272] 	 gamma: 0.990000
I0910 10:46:38.857577 140527592425472 dqn_agent.py:273] 	 update_horizon: 1.000000
I0910 10:46:38.857636 140527592425472 dqn_agent.py:274] 	 min_replay_history: 500
I0910 10:46:38.857690 140527592425472 dqn_agent.py:275] 	 update_period: 4
I0910 10:46:38.857743 140527592425472 dqn_agent.py:276] 	 target_update_period: 100
I0910 10:46:38.857836 140527592425472 dqn_agent.py:277] 	 epsilon_train: 0.010000
I0910 10:46:38.857916 140527592425472 dqn_agent.py:278] 	 epsilon_eval: 0.001000
I0910 10:46:38.857986 140527592425472 dqn_agent.py:279] 	 epsilon_decay_period: 250000
I0910 10:46:38.858053 140527592425472 dqn_agent.py:280] 	 optimizer: adam
I0910 10:46:38.858127 140527592425472 dqn_agent.py:282] 	 max_tf_checkpoints_to_keep: 4
I0910 10:46:38.858194 140527592425472 dqn_agent.py:283] 	 seed: 1631270798857283
I0910 10:46:38.859522 140527592425472 circular_replay_buffer.py:155] Creating a OutOfGraphPrioritizedReplayBuffer replay memory with the following parameters:
I0910 10:46:38.859645 140527592425472 circular_replay_buffer.py:156] 	 observation_shape: (4, 1)
I0910 10:46:38.859694 140527592425472 circular_replay_buffer.py:157] 	 observation_dtype: <class 'jax._src.numpy.lax_numpy.float64'>
I0910 10:46:38.859756 140527592425472 circular_replay_buffer.py:158] 	 terminal_dtype: <class 'numpy.uint8'>
I0910 10:46:38.859814 140527592425472 circular_replay_buffer.py:159] 	 stack_size: 1
I0910 10:46:38.859859 140527592425472 circular_replay_buffer.py:160] 	 replay_capacity: 50000
I0910 10:46:38.859903 140527592425472 circular_replay_buffer.py:161] 	 batch_size: 128
I0910 10:46:38.859964 140527592425472 circular_replay_buffer.py:162] 	 update_horizon: 1
I0910 10:46:38.860022 140527592425472 circular_replay_buffer.py:163] 	 gamma: 0.990000
I0910 10:46:38.877759 140527592425472 dqn_agent.py:70] Creating Adam optimizer with settings lr=0.001000, beta1=0.900000, beta2=0.999000, eps=0.000313
I0910 10:46:38.888490 140527592425472 run_experiment.py:516] Beginning training...
INFO:tensorflow:Starting iteration 0
I0910 10:46:38.888608 140527592425472 replay_runner.py:44] Starting iteration 0
INFO:tensorflow:Average training steps per second: 973834.22
I0910 10:46:38.889754 140527592425472 replay_runner.py:39] Average training steps per second: 973834.22
Steps executed: 319 Episode length: 157 Return: 157.0
I0910 10:46:39.509796 140527592425472 run_experiment.py:428] Average undiscounted return per evaluation episode: 159.50
Traceback (most recent call last):
  File "main_offline_experiments.py", line 133, in <module>
    app.run(main)
  File "/home/joaoguilhermearujo/miniconda3/envs/rain/lib/python3.6/site-packages/absl/app.py", line 312, in run
    _run_main(main, args)
  File "/home/joaoguilhermearujo/miniconda3/envs/rain/lib/python3.6/site-packages/absl/app.py", line 258, in _run_main
    sys.exit(main(argv))
  File "main_offline_experiments.py", line 125, in main
    offline_runner.run_experiment()
  File "/home/joaoguilhermearujo/miniconda3/envs/rain/lib/python3.6/site-packages/dopamine/discrete_domains/run_experiment.py", line 523, in run_experiment
    statistics = self._run_one_iteration(iteration)
  File "/home/joaoguilhermearujo/rainbow_extend/revisiting_rainbow/replay_runner.py", line 49, in _run_one_iteration
    self._save_tensorboard_summaries(iteration, num_episodes_eval, average_reward_eval)
  File "/home/joaoguilhermearujo/rainbow_extend/revisiting_rainbow/replay_runner.py", line 65, in _save_tensorboard_summaries
    self.wandb.log({
AttributeError: 'FixedReplayRunner' object has no attribute 'wandb'