I0910 11:00:38.278455 139949464582144 run_experiment.py:549] Creating TrainRunner ...
WARNING:tensorflow:From /home/joaoguilhermearujo/miniconda3/envs/rain/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
W0910 11:00:38.279042 139949464582144 deprecation.py:345] From /home/joaoguilhermearujo/miniconda3/envs/rain/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
I0910 11:00:38.338982 139949464582144 dqn_agent.py:271] Creating JaxDQNAgentNew agent with the following parameters:
I0910 11:00:38.340142 139949464582144 dqn_agent.py:272] 	 gamma: 0.990000
I0910 11:00:38.340343 139949464582144 dqn_agent.py:273] 	 update_horizon: 1.000000
I0910 11:00:38.340426 139949464582144 dqn_agent.py:274] 	 min_replay_history: 500
I0910 11:00:38.340488 139949464582144 dqn_agent.py:275] 	 update_period: 4
I0910 11:00:38.340545 139949464582144 dqn_agent.py:276] 	 target_update_period: 100
I0910 11:00:38.340598 139949464582144 dqn_agent.py:277] 	 epsilon_train: 0.010000
I0910 11:00:38.340699 139949464582144 dqn_agent.py:278] 	 epsilon_eval: 0.001000
I0910 11:00:38.340793 139949464582144 dqn_agent.py:279] 	 epsilon_decay_period: 250000
I0910 11:00:38.340865 139949464582144 dqn_agent.py:280] 	 optimizer: adam
I0910 11:00:38.340926 139949464582144 dqn_agent.py:282] 	 max_tf_checkpoints_to_keep: 4
I0910 11:00:38.340978 139949464582144 dqn_agent.py:283] 	 seed: 1631271638338925
I0910 11:00:38.342646 139949464582144 circular_replay_buffer.py:155] Creating a OutOfGraphPrioritizedReplayBuffer replay memory with the following parameters:
I0910 11:00:38.342788 139949464582144 circular_replay_buffer.py:156] 	 observation_shape: (2, 1)
I0910 11:00:38.342899 139949464582144 circular_replay_buffer.py:157] 	 observation_dtype: <class 'jax._src.numpy.lax_numpy.float64'>
I0910 11:00:38.342969 139949464582144 circular_replay_buffer.py:158] 	 terminal_dtype: <class 'numpy.uint8'>
I0910 11:00:38.343026 139949464582144 circular_replay_buffer.py:159] 	 stack_size: 1
I0910 11:00:38.343083 139949464582144 circular_replay_buffer.py:160] 	 replay_capacity: 50000
I0910 11:00:38.343134 139949464582144 circular_replay_buffer.py:161] 	 batch_size: 128
I0910 11:00:38.343294 139949464582144 circular_replay_buffer.py:162] 	 update_horizon: 1
I0910 11:00:38.343375 139949464582144 circular_replay_buffer.py:163] 	 gamma: 0.990000
Loaded trained dqn in mountaincar
Training fixed agent 2, please be patient, may be a while...
Steps executed: 600 Episode length: 600 Return: -600.0
I0910 11:00:39.785591 139949464582144 dqn_agent.py:70] Creating Adam optimizer with settings lr=0.010000, beta1=0.900000, beta2=0.999000, eps=0.000313
I0910 11:00:39.876754 139949464582144 dqn_agent.py:271] Creating JaxDQNAgentNew agent with the following parameters:
I0910 11:00:39.876965 139949464582144 dqn_agent.py:272] 	 gamma: 0.990000
I0910 11:00:39.877080 139949464582144 dqn_agent.py:273] 	 update_horizon: 1.000000
I0910 11:00:39.877145 139949464582144 dqn_agent.py:274] 	 min_replay_history: 500
I0910 11:00:39.877243 139949464582144 dqn_agent.py:275] 	 update_period: 4
I0910 11:00:39.877310 139949464582144 dqn_agent.py:276] 	 target_update_period: 100
I0910 11:00:39.877433 139949464582144 dqn_agent.py:277] 	 epsilon_train: 0.010000
I0910 11:00:39.877530 139949464582144 dqn_agent.py:278] 	 epsilon_eval: 0.001000
I0910 11:00:39.877593 139949464582144 dqn_agent.py:279] 	 epsilon_decay_period: 250000
I0910 11:00:39.877646 139949464582144 dqn_agent.py:280] 	 optimizer: adam
I0910 11:00:39.877697 139949464582144 dqn_agent.py:282] 	 max_tf_checkpoints_to_keep: 4
I0910 11:00:39.877749 139949464582144 dqn_agent.py:283] 	 seed: 1631271639876701
I0910 11:00:39.879503 139949464582144 circular_replay_buffer.py:155] Creating a OutOfGraphPrioritizedReplayBuffer replay memory with the following parameters:
I0910 11:00:39.879648 139949464582144 circular_replay_buffer.py:156] 	 observation_shape: (2, 1)
I0910 11:00:39.879721 139949464582144 circular_replay_buffer.py:157] 	 observation_dtype: <class 'jax._src.numpy.lax_numpy.float64'>
I0910 11:00:39.879817 139949464582144 circular_replay_buffer.py:158] 	 terminal_dtype: <class 'numpy.uint8'>
I0910 11:00:39.879900 139949464582144 circular_replay_buffer.py:159] 	 stack_size: 1
I0910 11:00:39.879959 139949464582144 circular_replay_buffer.py:160] 	 replay_capacity: 50000
I0910 11:00:39.880012 139949464582144 circular_replay_buffer.py:161] 	 batch_size: 128
I0910 11:00:39.880125 139949464582144 circular_replay_buffer.py:162] 	 update_horizon: 1
I0910 11:00:39.880199 139949464582144 circular_replay_buffer.py:163] 	 gamma: 0.990000
I0910 11:00:39.902490 139949464582144 dqn_agent.py:70] Creating Adam optimizer with settings lr=0.010000, beta1=0.900000, beta2=0.999000, eps=0.000313
I0910 11:00:39.916229 139949464582144 run_experiment.py:516] Beginning training...
INFO:tensorflow:Starting iteration 0
I0910 11:00:39.916425 139949464582144 replay_runner.py:44] Starting iteration 0
INFO:tensorflow:Average training steps per second: 709576.04
I0910 11:00:39.917961 139949464582144 replay_runner.py:39] Average training steps per second: 709576.04
I0910 11:00:40.706578 139949464582144 run_experiment.py:428] Average undiscounted return per evaluation episode: -600.00
INFO:tensorflow:Starting iteration 1
I0910 11:00:40.766538 139949464582144 replay_runner.py:44] Starting iteration 1
INFO:tensorflow:Average training steps per second: 804894.26
I0910 11:00:40.768049 139949464582144 replay_runner.py:39] Average training steps per second: 804894.26
I0910 11:00:40.940948 139949464582144 run_experiment.py:428] Average undiscounted return per evaluation episode: -600.00
INFO:tensorflow:Starting iteration 2
I0910 11:00:40.994994 139949464582144 replay_runner.py:44] Starting iteration 2
INFO:tensorflow:Average training steps per second: 767484.72
I0910 11:00:40.996560 139949464582144 replay_runner.py:39] Average training steps per second: 767484.72
I0910 11:00:41.165408 139949464582144 run_experiment.py:428] Average undiscounted return per evaluation episode: -600.00
INFO:tensorflow:Starting iteration 3
I0910 11:00:41.219541 139949464582144 replay_runner.py:44] Starting iteration 3
INFO:tensorflow:Average training steps per second: 811277.37
I0910 11:00:41.221012 139949464582144 replay_runner.py:39] Average training steps per second: 811277.37
I0910 11:00:41.399544 139949464582144 run_experiment.py:428] Average undiscounted return per evaluation episode: -600.00
INFO:tensorflow:Starting iteration 4
I0910 11:00:41.454913 139949464582144 replay_runner.py:44] Starting iteration 4
INFO:tensorflow:Average training steps per second: 731990.23
I0910 11:00:41.456533 139949464582144 replay_runner.py:39] Average training steps per second: 731990.23
I0910 11:00:41.633488 139949464582144 run_experiment.py:428] Average undiscounted return per evaluation episode: -600.00
INFO:tensorflow:Starting iteration 5
I0910 11:00:41.689074 139949464582144 replay_runner.py:44] Starting iteration 5
INFO:tensorflow:Average training steps per second: 758738.06
I0910 11:00:41.690680 139949464582144 replay_runner.py:39] Average training steps per second: 758738.06
I0910 11:00:41.872728 139949464582144 run_experiment.py:428] Average undiscounted return per evaluation episode: -600.00
INFO:tensorflow:Starting iteration 6
I0910 11:00:41.928789 139949464582144 replay_runner.py:44] Starting iteration 6
INFO:tensorflow:Average training steps per second: 828586.33
I0910 11:00:41.930242 139949464582144 replay_runner.py:39] Average training steps per second: 828586.33
I0910 11:00:42.121881 139949464582144 run_experiment.py:428] Average undiscounted return per evaluation episode: -600.00
INFO:tensorflow:Starting iteration 7
I0910 11:00:42.177129 139949464582144 replay_runner.py:44] Starting iteration 7
INFO:tensorflow:Average training steps per second: 825650.39
I0910 11:00:42.178588 139949464582144 replay_runner.py:39] Average training steps per second: 825650.39
I0910 11:00:42.402133 139949464582144 run_experiment.py:428] Average undiscounted return per evaluation episode: -600.00
INFO:tensorflow:Starting iteration 8
I0910 11:00:42.470752 139949464582144 replay_runner.py:44] Starting iteration 8
INFO:tensorflow:Average training steps per second: 795581.18
I0910 11:00:42.472245 139949464582144 replay_runner.py:39] Average training steps per second: 795581.18
I0910 11:00:42.647479 139949464582144 run_experiment.py:428] Average undiscounted return per evaluation episode: -600.00
INFO:tensorflow:Starting iteration 9
I0910 11:00:42.703973 139949464582144 replay_runner.py:44] Starting iteration 9
INFO:tensorflow:Average training steps per second: 822251.32
I0910 11:00:42.705441 139949464582144 replay_runner.py:39] Average training steps per second: 822251.32
I0910 11:00:42.911601 139949464582144 run_experiment.py:428] Average undiscounted return per evaluation episode: -600.00
INFO:tensorflow:Starting iteration 10
I0910 11:00:42.970007 139949464582144 replay_runner.py:44] Starting iteration 10
INFO:tensorflow:Average training steps per second: 768328.27
I0910 11:00:42.971569 139949464582144 replay_runner.py:39] Average training steps per second: 768328.27
I0910 11:00:43.167590 139949464582144 run_experiment.py:428] Average undiscounted return per evaluation episode: -600.00
INFO:tensorflow:Starting iteration 11
I0910 11:00:43.256716 139949464582144 replay_runner.py:44] Starting iteration 11
INFO:tensorflow:Average training steps per second: 340391.49
I0910 11:00:43.260055 139949464582144 replay_runner.py:39] Average training steps per second: 340391.49
I0910 11:00:43.455380 139949464582144 run_experiment.py:428] Average undiscounted return per evaluation episode: -600.00
INFO:tensorflow:Starting iteration 12
I0910 11:00:43.513917 139949464582144 replay_runner.py:44] Starting iteration 12
INFO:tensorflow:Average training steps per second: 788551.23
I0910 11:00:43.515444 139949464582144 replay_runner.py:39] Average training steps per second: 788551.23
I0910 11:00:43.709899 139949464582144 run_experiment.py:428] Average undiscounted return per evaluation episode: -600.00
INFO:tensorflow:Starting iteration 13
I0910 11:00:43.766308 139949464582144 replay_runner.py:44] Starting iteration 13
INFO:tensorflow:Average training steps per second: 825163.09
I0910 11:00:43.767757 139949464582144 replay_runner.py:39] Average training steps per second: 825163.09
I0910 11:00:43.991367 139949464582144 run_experiment.py:428] Average undiscounted return per evaluation episode: -600.00
INFO:tensorflow:Starting iteration 14
I0910 11:00:44.051778 139949464582144 replay_runner.py:44] Starting iteration 14
INFO:tensorflow:Average training steps per second: 838357.79
I0910 11:00:44.053243 139949464582144 replay_runner.py:39] Average training steps per second: 838357.79
Traceback (most recent call last):
  File "main_offline_experiments.py", line 131, in <module>
    app.run(main)
  File "/home/joaoguilhermearujo/miniconda3/envs/rain/lib/python3.6/site-packages/absl/app.py", line 312, in run
    _run_main(main, args)
  File "/home/joaoguilhermearujo/miniconda3/envs/rain/lib/python3.6/site-packages/absl/app.py", line 258, in _run_main
    sys.exit(main(argv))
  File "main_offline_experiments.py", line 123, in main
    offline_runner.run_experiment()
  File "/home/joaoguilhermearujo/miniconda3/envs/rain/lib/python3.6/site-packages/dopamine/discrete_domains/run_experiment.py", line 523, in run_experiment
    statistics = self._run_one_iteration(iteration)
  File "/home/joaoguilhermearujo/rainbow_extend/revisiting_rainbow/replay_runner.py", line 47, in _run_one_iteration
    num_episodes_eval, average_reward_eval = self._run_eval_phase(statistics)
  File "/home/joaoguilhermearujo/miniconda3/envs/rain/lib/python3.6/site-packages/dopamine/discrete_domains/run_experiment.py", line 425, in _run_eval_phase
    self._evaluation_steps, statistics, 'eval')
  File "/home/joaoguilhermearujo/miniconda3/envs/rain/lib/python3.6/site-packages/dopamine/discrete_domains/run_experiment.py", line 366, in _run_one_phase
    episode_length, episode_return = self._run_one_episode()
  File "/home/joaoguilhermearujo/miniconda3/envs/rain/lib/python3.6/site-packages/dopamine/discrete_domains/run_experiment.py", line 339, in _run_one_episode
    action = self._agent.step(reward, observation)
  File "/home/joaoguilhermearujo/rainbow_extend/revisiting_rainbow/agents/dqn_agent_new.py", line 394, in step
    self.epsilon_fn)
ValueError: Non-hashable static arguments are not supported. An error occured while trying to hash an object of type <class 'agents.networks_new.DQNNetwork'>, DQNNetwork(). The error was:
KeyboardInterrupt:
At:
  /home/joaoguilhermearujo/miniconda3/envs/rain/lib/python3.6/site-packages/flax/linen/module.py(295): wrapped
  /home/joaoguilhermearujo/rainbow_extend/revisiting_rainbow/agents/dqn_agent_new.py(394): step
  /home/joaoguilhermearujo/miniconda3/envs/rain/lib/python3.6/site-packages/dopamine/discrete_domains/run_experiment.py(339): _run_one_episode
  /home/joaoguilhermearujo/miniconda3/envs/rain/lib/python3.6/site-packages/dopamine/discrete_domains/run_experiment.py(366): _run_one_phase
  /home/joaoguilhermearujo/miniconda3/envs/rain/lib/python3.6/site-packages/dopamine/discrete_domains/run_experiment.py(425): _run_eval_phase
  /home/joaoguilhermearujo/rainbow_extend/revisiting_rainbow/replay_runner.py(47): _run_one_iteration
  /home/joaoguilhermearujo/miniconda3/envs/rain/lib/python3.6/site-packages/dopamine/discrete_domains/run_experiment.py(523): run_experiment
  main_offline_experiments.py(123): main
  /home/joaoguilhermearujo/miniconda3/envs/rain/lib/python3.6/site-packages/absl/app.py(258): _run_main
  /home/joaoguilhermearujo/miniconda3/envs/rain/lib/python3.6/site-packages/absl/app.py(312): run
  main_offline_experiments.py(131): <module>