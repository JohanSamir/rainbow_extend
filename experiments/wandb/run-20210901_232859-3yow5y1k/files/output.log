I0901 23:29:05.694029 140574080804864 run_experiment.py:549] Creating TrainRunner ...
WARNING:tensorflow:From /home/joaoguilhermearujo/miniconda3/envs/rain/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
W0901 23:29:05.694543 140574080804864 deprecation.py:345] From /home/joaoguilhermearujo/miniconda3/envs/rain/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
I0901 23:29:05.753056 140574080804864 dqn_agent.py:271] Creating JaxDQNAgentNew agent with the following parameters:
I0901 23:29:05.754132 140574080804864 dqn_agent.py:272] 	 gamma: 0.990000
I0901 23:29:05.754281 140574080804864 dqn_agent.py:273] 	 update_horizon: 1.000000
I0901 23:29:05.754387 140574080804864 dqn_agent.py:274] 	 min_replay_history: 500
I0901 23:29:05.754483 140574080804864 dqn_agent.py:275] 	 update_period: 4
I0901 23:29:05.754573 140574080804864 dqn_agent.py:276] 	 target_update_period: 100
I0901 23:29:05.754692 140574080804864 dqn_agent.py:277] 	 epsilon_train: 0.010000
I0901 23:29:05.754820 140574080804864 dqn_agent.py:278] 	 epsilon_eval: 0.001000
I0901 23:29:05.754973 140574080804864 dqn_agent.py:279] 	 epsilon_decay_period: 250000
I0901 23:29:05.755086 140574080804864 dqn_agent.py:280] 	 optimizer: adam
I0901 23:29:05.755204 140574080804864 dqn_agent.py:282] 	 max_tf_checkpoints_to_keep: 4
I0901 23:29:05.755311 140574080804864 dqn_agent.py:283] 	 seed: 1630538945753010
I0901 23:29:05.758098 140574080804864 circular_replay_buffer.py:155] Creating a OutOfGraphPrioritizedReplayBuffer replay memory with the following parameters:
I0901 23:29:05.758278 140574080804864 circular_replay_buffer.py:156] 	 observation_shape: (4, 1)
I0901 23:29:05.758393 140574080804864 circular_replay_buffer.py:157] 	 observation_dtype: <class 'jax._src.numpy.lax_numpy.float64'>
I0901 23:29:05.758499 140574080804864 circular_replay_buffer.py:158] 	 terminal_dtype: <class 'numpy.uint8'>
I0901 23:29:05.758596 140574080804864 circular_replay_buffer.py:159] 	 stack_size: 1
I0901 23:29:05.758704 140574080804864 circular_replay_buffer.py:160] 	 replay_capacity: 50000
I0901 23:29:05.758816 140574080804864 circular_replay_buffer.py:161] 	 batch_size: 128
I0901 23:29:05.758965 140574080804864 circular_replay_buffer.py:162] 	 update_horizon: 1
I0901 23:29:05.759076 140574080804864 circular_replay_buffer.py:163] 	 gamma: 0.990000
I0901 23:29:10.759111 140574080804864 dqn_agent.py:70] Creating Adam optimizer with settings lr=0.001000, beta1=0.900000, beta2=0.999000, eps=0.000313
Loaded trained dqn in cartpole
Training fixed agent 1, please be patient, may be a while...
I0901 23:29:12.162039 140574080804864 dqn_agent.py:70] Creating Adam optimizer with settings lr=0.001000, beta1=0.900000, beta2=0.999000, eps=0.000313
I0901 23:29:12.170423 140574080804864 run_experiment.py:269] Reloaded checkpoint and will start from iteration 30
I0901 23:29:12.175193 140574080804864 dqn_agent.py:271] Creating JaxDQNAgentNew agent with the following parameters:
I0901 23:29:12.175335 140574080804864 dqn_agent.py:272] 	 gamma: 0.990000
I0901 23:29:12.175450 140574080804864 dqn_agent.py:273] 	 update_horizon: 1.000000
I0901 23:29:12.175539 140574080804864 dqn_agent.py:274] 	 min_replay_history: 500
I0901 23:29:12.175626 140574080804864 dqn_agent.py:275] 	 update_period: 4
I0901 23:29:12.175728 140574080804864 dqn_agent.py:276] 	 target_update_period: 100
I0901 23:29:12.175815 140574080804864 dqn_agent.py:277] 	 epsilon_train: 0.010000
I0901 23:29:12.175907 140574080804864 dqn_agent.py:278] 	 epsilon_eval: 0.001000
I0901 23:29:12.175985 140574080804864 dqn_agent.py:279] 	 epsilon_decay_period: 250000
I0901 23:29:12.176068 140574080804864 dqn_agent.py:280] 	 optimizer: adam
I0901 23:29:12.176147 140574080804864 dqn_agent.py:282] 	 max_tf_checkpoints_to_keep: 4
I0901 23:29:12.176225 140574080804864 dqn_agent.py:283] 	 seed: 1630538952175162
I0901 23:29:12.177865 140574080804864 circular_replay_buffer.py:155] Creating a OutOfGraphPrioritizedReplayBuffer replay memory with the following parameters:
I0901 23:29:12.177989 140574080804864 circular_replay_buffer.py:156] 	 observation_shape: (4, 1)
I0901 23:29:12.178063 140574080804864 circular_replay_buffer.py:157] 	 observation_dtype: <class 'jax._src.numpy.lax_numpy.float64'>
I0901 23:29:12.178138 140574080804864 circular_replay_buffer.py:158] 	 terminal_dtype: <class 'numpy.uint8'>
I0901 23:29:12.178194 140574080804864 circular_replay_buffer.py:159] 	 stack_size: 1
I0901 23:29:12.178261 140574080804864 circular_replay_buffer.py:160] 	 replay_capacity: 50000
I0901 23:29:12.178339 140574080804864 circular_replay_buffer.py:161] 	 batch_size: 128
I0901 23:29:12.178412 140574080804864 circular_replay_buffer.py:162] 	 update_horizon: 1
I0901 23:29:12.178484 140574080804864 circular_replay_buffer.py:163] 	 gamma: 0.990000
I0901 23:29:12.585547 140574080804864 dqn_agent.py:70] Creating Adam optimizer with settings lr=0.001000, beta1=0.900000, beta2=0.999000, eps=0.000313
I0901 23:29:12.701264 140574080804864 dqn_agent.py:70] Creating Adam optimizer with settings lr=0.001000, beta1=0.900000, beta2=0.999000, eps=0.000313
I0901 23:29:12.790165 140574080804864 run_experiment.py:269] Reloaded checkpoint and will start from iteration 19
I0901 23:29:12.791885 140574080804864 run_experiment.py:516] Beginning training...
INFO:tensorflow:Starting iteration 19
I0901 23:29:12.792023 140574080804864 replay_runner.py:41] Starting iteration 19
INFO:tensorflow:Average training steps per second: 104.45
I0901 23:29:22.366571 140574080804864 replay_runner.py:36] Average training steps per second: 104.45
Steps executed: 321 Episode length: 170 Return: 170.0
I0901 23:29:23.528209 140574080804864 run_experiment.py:428] Average undiscounted return per evaluation episode: 160.50
INFO:tensorflow:Starting iteration 20

Steps executed: 200 Episode length: 200 Return: 200.0
INFO:tensorflow:Average training steps per second: 210.05
I0901 23:29:28.457646 140574080804864 replay_runner.py:36] Average training steps per second: 210.05
I0901 23:29:28.615249 140574080804864 run_experiment.py:428] Average undiscounted return per evaluation episode: 200.00
INFO:tensorflow:Starting iteration 21

Steps executed: 374 Episode length: 189 Return: 189.0
INFO:tensorflow:Average training steps per second: 185.88
I0901 23:29:34.203550 140574080804864 replay_runner.py:36] Average training steps per second: 185.88
I0901 23:29:34.438936 140574080804864 run_experiment.py:428] Average undiscounted return per evaluation episode: 187.00
INFO:tensorflow:Starting iteration 22

Steps executed: 200 Episode length: 200 Return: 200.0
INFO:tensorflow:Average training steps per second: 196.49
I0901 23:29:39.714982 140574080804864 replay_runner.py:36] Average training steps per second: 196.49
I0901 23:29:39.870540 140574080804864 run_experiment.py:428] Average undiscounted return per evaluation episode: 200.00
INFO:tensorflow:Starting iteration 23
I0901 23:29:40.078861 140574080804864 replay_runner.py:41] Starting iteration 23
INFO:tensorflow:Average training steps per second: 194.26
I0901 23:29:45.227062 140574080804864 replay_runner.py:36] Average training steps per second: 194.26
I0901 23:29:45.356079 140574080804864 run_experiment.py:428] Average undiscounted return per evaluation episode: 200.00
INFO:tensorflow:Starting iteration 24
I0901 23:29:45.541006 140574080804864 replay_runner.py:41] Starting iteration 24
INFO:tensorflow:Average training steps per second: 189.74
I0901 23:29:50.812020 140574080804864 replay_runner.py:36] Average training steps per second: 189.74
I0901 23:29:50.959927 140574080804864 run_experiment.py:428] Average undiscounted return per evaluation episode: 200.00
INFO:tensorflow:Starting iteration 25
I0901 23:29:51.166782 140574080804864 replay_runner.py:41] Starting iteration 25
INFO:tensorflow:Average training steps per second: 198.02
I0901 23:29:56.217469 140574080804864 replay_runner.py:36] Average training steps per second: 198.02
I0901 23:29:56.359820 140574080804864 run_experiment.py:428] Average undiscounted return per evaluation episode: 200.00
INFO:tensorflow:Starting iteration 26

Steps executed: 331 Episode length: 161 Return: 161.0
INFO:tensorflow:Average training steps per second: 192.61
I0901 23:30:01.744621 140574080804864 replay_runner.py:36] Average training steps per second: 192.61
I0901 23:30:01.984560 140574080804864 run_experiment.py:428] Average undiscounted return per evaluation episode: 165.50
INFO:tensorflow:Starting iteration 27

Steps executed: 326 Episode length: 162 Return: 162.0
INFO:tensorflow:Average training steps per second: 204.88
I0901 23:30:07.063789 140574080804864 replay_runner.py:36] Average training steps per second: 204.88
I0901 23:30:07.271538 140574080804864 run_experiment.py:428] Average undiscounted return per evaluation episode: 163.00
INFO:tensorflow:Starting iteration 28

Steps executed: 375 Episode length: 195 Return: 195.0
INFO:tensorflow:Average training steps per second: 196.18
I0901 23:30:12.558598 140574080804864 replay_runner.py:36] Average training steps per second: 196.18
I0901 23:30:12.828432 140574080804864 run_experiment.py:428] Average undiscounted return per evaluation episode: 187.50
INFO:tensorflow:Starting iteration 29

Done fixed training!Episode length: 200 Return: 200.0
INFO:tensorflow:Average training steps per second: 201.69
I0901 23:30:17.990706 140574080804864 replay_runner.py:36] Average training steps per second: 201.69
I0901 23:30:18.127175 140574080804864 run_experiment.py:428] Average undiscounted return per evaluation episode: 200.00