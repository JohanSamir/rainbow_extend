I0828 10:16:26.294739 140022852646912 run_experiment.py:549] Creating TrainRunner ...
WARNING:tensorflow:From /home/joaoguilhermearujo/miniconda3/envs/rain/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
W0828 10:16:26.295163 140022852646912 deprecation.py:345] From /home/joaoguilhermearujo/miniconda3/envs/rain/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
I0828 10:16:26.345683 140022852646912 dqn_agent.py:271] Creating JaxDQNAgentNew agent with the following parameters:
I0828 10:16:26.346427 140022852646912 dqn_agent.py:272] 	 gamma: 0.990000
I0828 10:16:26.346482 140022852646912 dqn_agent.py:273] 	 update_horizon: 1.000000
I0828 10:16:26.346530 140022852646912 dqn_agent.py:274] 	 min_replay_history: 500
I0828 10:16:26.346587 140022852646912 dqn_agent.py:275] 	 update_period: 4
I0828 10:16:26.346635 140022852646912 dqn_agent.py:276] 	 target_update_period: 100
I0828 10:16:26.346688 140022852646912 dqn_agent.py:277] 	 epsilon_train: 0.010000
I0828 10:16:26.346733 140022852646912 dqn_agent.py:278] 	 epsilon_eval: 0.001000
I0828 10:16:26.346811 140022852646912 dqn_agent.py:279] 	 epsilon_decay_period: 250000
I0828 10:16:26.346890 140022852646912 dqn_agent.py:280] 	 optimizer: adam
I0828 10:16:26.346955 140022852646912 dqn_agent.py:282] 	 max_tf_checkpoints_to_keep: 4
I0828 10:16:26.347010 140022852646912 dqn_agent.py:283] 	 seed: 1630145786345644
I0828 10:16:26.348309 140022852646912 circular_replay_buffer.py:155] Creating a OutOfGraphPrioritizedReplayBuffer replay memory with the following parameters:
I0828 10:16:26.348441 140022852646912 circular_replay_buffer.py:156] 	 observation_shape: (4, 1)
I0828 10:16:26.348510 140022852646912 circular_replay_buffer.py:157] 	 observation_dtype: <class 'jax._src.numpy.lax_numpy.float64'>
I0828 10:16:26.348565 140022852646912 circular_replay_buffer.py:158] 	 terminal_dtype: <class 'numpy.uint8'>
I0828 10:16:26.348617 140022852646912 circular_replay_buffer.py:159] 	 stack_size: 1
I0828 10:16:26.348668 140022852646912 circular_replay_buffer.py:160] 	 replay_capacity: 50000
I0828 10:16:26.348733 140022852646912 circular_replay_buffer.py:161] 	 batch_size: 128
I0828 10:16:26.348796 140022852646912 circular_replay_buffer.py:162] 	 update_horizon: 1
I0828 10:16:26.348846 140022852646912 circular_replay_buffer.py:163] 	 gamma: 0.990000
I0828 10:16:27.542811 140022852646912 dqn_agent.py:70] Creating Adam optimizer with settings lr=10.000000, beta1=0.900000, beta2=0.999000, eps=0.000313
Loaded trained dqn in cartpole
Training fixed agent 1, please be patient, may be a while...
I0828 10:16:27.865125 140022852646912 dqn_agent.py:70] Creating Adam optimizer with settings lr=10.000000, beta1=0.900000, beta2=0.999000, eps=0.000313
I0828 10:16:27.873419 140022852646912 run_experiment.py:269] Reloaded checkpoint and will start from iteration 30
I0828 10:16:27.878128 140022852646912 dqn_agent.py:271] Creating JaxDQNAgentNew agent with the following parameters:
I0828 10:16:27.878257 140022852646912 dqn_agent.py:272] 	 gamma: 0.990000
I0828 10:16:27.878330 140022852646912 dqn_agent.py:273] 	 update_horizon: 1.000000
I0828 10:16:27.878381 140022852646912 dqn_agent.py:274] 	 min_replay_history: 500
I0828 10:16:27.878428 140022852646912 dqn_agent.py:275] 	 update_period: 4
I0828 10:16:27.878479 140022852646912 dqn_agent.py:276] 	 target_update_period: 100
I0828 10:16:27.878528 140022852646912 dqn_agent.py:277] 	 epsilon_train: 0.010000
I0828 10:16:27.878582 140022852646912 dqn_agent.py:278] 	 epsilon_eval: 0.001000
I0828 10:16:27.878650 140022852646912 dqn_agent.py:279] 	 epsilon_decay_period: 250000
I0828 10:16:27.878740 140022852646912 dqn_agent.py:280] 	 optimizer: adam
I0828 10:16:27.878817 140022852646912 dqn_agent.py:282] 	 max_tf_checkpoints_to_keep: 4
I0828 10:16:27.878869 140022852646912 dqn_agent.py:283] 	 seed: 1630145787878080
I0828 10:16:27.880270 140022852646912 circular_replay_buffer.py:155] Creating a OutOfGraphPrioritizedReplayBuffer replay memory with the following parameters:
I0828 10:16:27.880405 140022852646912 circular_replay_buffer.py:156] 	 observation_shape: (4, 1)
I0828 10:16:27.880478 140022852646912 circular_replay_buffer.py:157] 	 observation_dtype: <class 'jax._src.numpy.lax_numpy.float64'>
I0828 10:16:27.880538 140022852646912 circular_replay_buffer.py:158] 	 terminal_dtype: <class 'numpy.uint8'>
I0828 10:16:27.880591 140022852646912 circular_replay_buffer.py:159] 	 stack_size: 1
I0828 10:16:27.880645 140022852646912 circular_replay_buffer.py:160] 	 replay_capacity: 50000
I0828 10:16:27.880712 140022852646912 circular_replay_buffer.py:161] 	 batch_size: 128
I0828 10:16:27.880782 140022852646912 circular_replay_buffer.py:162] 	 update_horizon: 1
I0828 10:16:27.880843 140022852646912 circular_replay_buffer.py:163] 	 gamma: 0.990000
I0828 10:16:27.899764 140022852646912 dqn_agent.py:70] Creating Adam optimizer with settings lr=10.000000, beta1=0.900000, beta2=0.999000, eps=0.000313
I0828 10:16:27.938143 140022852646912 dqn_agent.py:70] Creating Adam optimizer with settings lr=10.000000, beta1=0.900000, beta2=0.999000, eps=0.000313
I0828 10:16:27.945420 140022852646912 run_experiment.py:269] Reloaded checkpoint and will start from iteration 22
I0828 10:16:27.945557 140022852646912 run_experiment.py:516] Beginning training...
INFO:tensorflow:Starting iteration 22
I0828 10:16:27.945630 140022852646912 replay_runner.py:41] Starting iteration 22
INFO:tensorflow:Average training steps per second: 283.58
I0828 10:16:31.472122 140022852646912 replay_runner.py:36] Average training steps per second: 283.58
Steps executed: 213 Episode length: 18 Return: 18.0
I0828 10:16:32.019150 140022852646912 run_experiment.py:428] Average undiscounted return per evaluation episode: 35.50
INFO:tensorflow:Starting iteration 23

Steps executed: 265 Episode length: 89 Return: 89.0
INFO:tensorflow:Average training steps per second: 365.74
I0828 10:16:34.866699 140022852646912 replay_runner.py:36] Average training steps per second: 365.74
I0828 10:16:34.949210 140022852646912 run_experiment.py:428] Average undiscounted return per evaluation episode: 88.33
INFO:tensorflow:Starting iteration 24

Steps executed: 226 Episode length: 59 Return: 59.0
INFO:tensorflow:Average training steps per second: 370.28
I0828 10:16:37.763182 140022852646912 replay_runner.py:36] Average training steps per second: 370.28
I0828 10:16:37.831937 140022852646912 run_experiment.py:428] Average undiscounted return per evaluation episode: 56.50
INFO:tensorflow:Starting iteration 25

Steps executed: 200 Episode length: 88 Return: 88.0.0
INFO:tensorflow:Average training steps per second: 364.83
I0828 10:16:40.684344 140022852646912 replay_runner.py:36] Average training steps per second: 364.83
I0828 10:16:40.754171 140022852646912 run_experiment.py:428] Average undiscounted return per evaluation episode: 100.00
INFO:tensorflow:Starting iteration 26

Steps executed: 200 Episode length: 200 Return: 200.0
INFO:tensorflow:Average training steps per second: 368.24
I0828 10:16:43.581897 140022852646912 replay_runner.py:36] Average training steps per second: 368.24
I0828 10:16:43.644491 140022852646912 run_experiment.py:428] Average undiscounted return per evaluation episode: 200.00
INFO:tensorflow:Starting iteration 27
I0828 10:16:43.754837 140022852646912 replay_runner.py:41] Starting iteration 27
INFO:tensorflow:Average training steps per second: 371.09
I0828 10:16:46.449869 140022852646912 replay_runner.py:36] Average training steps per second: 371.09
I0828 10:16:46.515419 140022852646912 run_experiment.py:428] Average undiscounted return per evaluation episode: 200.00
INFO:tensorflow:Starting iteration 28
I0828 10:16:46.626376 140022852646912 replay_runner.py:41] Starting iteration 28
INFO:tensorflow:Average training steps per second: 372.31
I0828 10:16:49.312462 140022852646912 replay_runner.py:36] Average training steps per second: 372.31
I0828 10:16:49.379624 140022852646912 run_experiment.py:428] Average undiscounted return per evaluation episode: 200.00
INFO:tensorflow:Starting iteration 29

Done fixed training!Episode length: 200 Return: 200.0
INFO:tensorflow:Average training steps per second: 374.96
I0828 10:16:52.162078 140022852646912 replay_runner.py:36] Average training steps per second: 374.96
I0828 10:16:52.226313 140022852646912 run_experiment.py:428] Average undiscounted return per evaluation episode: 200.00